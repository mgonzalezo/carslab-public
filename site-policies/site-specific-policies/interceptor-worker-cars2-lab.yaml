---
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "interceptor-worker"
  namespace: "ztp-site-cars2"
spec:
  bindingRules:
    sites: "interceptor-worker-cars2-lab"
  remediationAction: enforce
  mcp: "worker"
  sourceFiles:
    - fileName: MachineConfigGeneric.yaml 
      policyName: "config-policy"
      metadata:
        labels:
          machineconfiguration.openshift.io/role: worker
        name: enable-workload-partitioning
      spec:
        config:
          storage:
            files:
            - contents:
                source: data:text/plain;charset=utf-8;base64,W2NyaW8ucnVudGltZS53b3JrbG9hZHMubWFuYWdlbWVudF0KYWN0aXZhdGlvbl9hbm5vdGF0aW9uID0gInRhcmdldC53b3JrbG9hZC5vcGVuc2hpZnQuaW8vbWFuYWdlbWVudCIKYW5ub3RhdGlvbl9wcmVmaXggPSAicmVzb3VyY2VzLndvcmtsb2FkLm9wZW5zaGlmdC5pbyIKcmVzb3VyY2VzID0geyAiY3B1c2hhcmVzIiA9IDAsICJjcHVzZXQiID0gIjAtMyIgfQo=
              mode: 420
              overwrite: true
              path: /etc/crio/crio.conf.d/01-workload-partitioning
              user:
                name: root
            - contents:
                source: data:text/plain;charset=utf-8;base64,ewogICJtYW5hZ2VtZW50IjogewogICAgImNwdXNldCI6ICIwLTMiCiAgfQp9Cg==
              mode: 420
              overwrite: true
              path: /etc/kubernetes/openshift-workload-pinning
              user:
                name: root
    - fileName: PerformanceProfile.yaml
      policyName: "config-policy"
      metadata:
        name: "openshift-worker-node-performance-profile"
      spec:
        cpu:
          isolated: "2-31,34-63"
          reserved: "0-1,32-33"
        hugepages:
          defaultHugepagesSize: 1G
          pages:
            - size: 1G
              count: 32
        realTimeKernel:
          enabled: true
        workloadHints:
          realTime: true
          highPowerConsumption: false
          perPodPowerManagement: true
    - fileName: TunedPerformancePatch.yaml
      policyName: "config-policy"
      metadata:
        name: performance-patch-worker
      spec:
        profile:
          - name: performance-patch-worker
            data: |
              [main]
              summary=Configuration changes profile inherited from performance created tuned
              include=openshift-node-performance-openshift-worker-node-performance-profile
              [sysctl]
              kernel.timer_migration=1
              [scheduler]
              group.ice-ptp=0:f:10:*:ice-ptp.*
              group.ice-gnss=0:f:10:*:ice-gnss.*
              [service]
              service.stalld=start,enable
              service.chronyd=stop,disable
              [sysfs]
              /sys/devices/system/cpu/intel_pstate/max_perf_pct = 70
        recommend:
        - profile: performance-patch-worker
    - fileName: PtpConfigSlaveCvl.yaml
      policyName: "config-policy"
      metadata:
        name: "du-ptp-worker-slave"
      spec:
        profile:
        - name: "worker-slave"
          # This interface must match the hardware in this group
          interface: "ens2f0"
          ptp4lOpts: "-2 -s --summary_interval -4"
          phc2sysOpts: "-a -r -n 24"
        recommend:
        - profile: "worker-slave"
          priority: 5
          match:
          - nodeName: "discrete.interceptor.cars2.lab"
    - fileName: SriovFecClusterConfig.yaml
      policyName: "fec-policy"
      metadata:
        name: worker-config
      spec:
        drainSkip: true
        nodeSelector:
          node-role.kubernetes.io/worker: ""
        acceleratorSelector:
          pciAddress: 0000:f7:00.0
        physicalFunction:
         bbDevConfig:
          acc200:
            # Programming mode: 0 = VF Programming, 1 = PF Programming
            pfMode: false
            numVfBundles: 16
            maxQueueSize: 1024
            uplink4G:
              numQueueGroups: 0
              numAqsPerGroups: 16
              aqDepthLog2: 4
            downlink4G:
              numQueueGroups: 0
              numAqsPerGroups: 16
              aqDepthLog2: 4
            uplink5G:
              numQueueGroups: 4
              numAqsPerGroups: 16
              aqDepthLog2: 4
            downlink5G:
              numQueueGroups: 4
              numAqsPerGroups: 16
              aqDepthLog2: 4
            qfft:
              aqDepthLog2: 4
              numAqsPerGroups: 16
              numQueueGroups: 4
    - fileName: SriovNetworkNodePolicy.yaml
      remediationAction: enforce
      policyName: "sriovw-vfio-policy-ens2f0"
      metadata:
        name: "sriovw-nnp-vfio-ens2f0"
      spec:
        deviceType: vfio-pci
        mtu: 9216
        isRdma: false
        nicSelector:
          vendor: "8086"
          deviceID: "1593"
          pfNames: ["ens2f0#0-3"]
        numVfs: 8
        priority: 10
        resourceName: pci_sriov_net_ens2f0_0
    - fileName: SriovNetworkNodePolicy.yaml
      policyName: "sriovw-netd-policy-ens2f0"
      remediationAction: enforce
      metadata:
        name: "sriovw-nnp-netd-ens2f0"
      spec:
        deviceType: netdevice
        mtu: 9216
        isRdma: false
        nicSelector:
          vendor: "8086"
          deviceID: "1593"
          pfNames: ["ens2f0#4-7"]
        numVfs: 8
        priority: 10
        resourceName: pci_sriov_net_ens2f0_1
    - fileName: SriovNetwork.yaml
      policyName: "sriovw-network-policy-mplane"
      metadata:
        name: "sriovw-nw-vran-mp1"
      spec:
        resourceName: pci_sriov_net_ens2f0_1
        networkNamespace: flexranl1
        vlan: 3812
        ipam: |-
          {"type": "static"}
        linkState: auto
    - fileName: SriovNetwork.yaml
      policyName: "sriovw-network-policy-splane"
      metadata:
        name: "sriovw-nw-vran-splane1"
      spec:
        resourceName: pci_sriov_net_ens2f0_1
        networkNamespace: flexranl1
        vlan: 400
        ipam: |-
          {"type": "static"}
        linkState: auto
    - fileName: SriovNetwork.yaml
      policyName: "sriovw-network-policy-fh"
      metadata:
        name: "sriovw-nw-vran-fh1"
      spec:
        resourceName: pci_sriov_net_ens2f0_0
        networkNamespace: flexranl1
        vlan: 3811
        ipam: |-
          {"type": "static"}
        linkState: auto
